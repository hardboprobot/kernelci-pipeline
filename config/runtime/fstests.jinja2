{# -*- mode: Python -*- -#}
{# SPDX-License-Identifier: LGPL-2.1-or-later -#}

{%- extends 'base/fstests.jinja2' %}

{%- block python_globals %}
{{ super() }}
GCE               = '{{gce|default("")}}'
MAX_SHARDS        = '{{max_shards|default("")}}'
NJOBS             = '{{njobs|default("1")}}'
OUTPUT            = '{{output if output is not none else ""}}'
SSH_HOST          = '{{ssh_host if ssh_host is not none else ""}}'
SSH_KEY           = '{{ssh_key if ssh_key is not none else ""}}'
SSH_PORT          = '{{ssh_port if ssh_port is not none else ""}}'
SSH_USER          = '{{ssh_user if ssh_user is not none else ""}}'
TESTCASE          = '{{testcase|default("")}}'
TESTCFG           = '{{testcfg|default("")}}'
TESTGROUP         = '{{testgroup|default("")}}'
{% endblock %}

{%- block python_imports %}
{{ super() }}
import json
import os
import shutil
import subprocess
import xml.etree.ElementTree as ET
import re
import traceback
import time
import glob
from abc import ABC, abstractmethod
import datetime
import random
import string
import math
import threading
import tempfile
{%- endblock %}

{% block python_job -%}

# Last tested with xfstests-bld commit:
# 920c0e32622e85615810d3da0fae27cf7ea0f69e (Nov 28, 2022)

POLL_PERIOD      = 10   # seconds

class Fstests(ABC):
    """Base class (abstract) to handle the setup, running and collection of
    results for fstests.
    """
    def kernel_config(self, kdir):
        """Uses the kvm-xfstests script to configure a kernel for fstests. The
        kernel source must be already decompressed in KDIR.

        Args:
            kdir (str): base directory of the kernel source

        Returns:
            bool: True for success, False otherwise
        """
        try:
            os.chdir(kdir)
            result = subprocess.run(['kvm-xfstests', 'install-kconfig'], check=True)
            print(f"Kernel config done.")
            return True
        except Exception as e:
            print("Kernel config error:", e)
            return False

    def kernel_build(self, kdir, njobs=1):
        """Builds an already configured Linux kernel.

        Args:
            kdir (str): base directory of the kernel source
            njobs (int, optional): number of parallel build jobs

        Returns:
            True if the build process finished successfully, False otherwise.
        """
        try:
            os.chdir(kdir)
            result = subprocess.run(['make', '-j', njobs], check=True)
            print("Kernel build done.")
            return True
        except Exception as e:
            print("Kernel build error:", e)
            return False

    def _parse_xml_test_results(self, f, tree):
        """Parses the fstests XML-formatted test results into an in-memory
        tree (dict).

        Args:
            f (str): path of the XML file to parse
            tree (dict): dict to save the data to. It may be populated
                already.

        Returns:
            A dict containing the result tree.
        """
        xml_file = ET.parse(f)
        root = xml_file.getroot()

        # Parse the test config: filesystem type and config, create the
        # initial tree nodes for them if they don't exist yet
        testcfg = root.find('properties/property[@name="TESTCFG"]').attrib['value']
        fs, cfg = testcfg.split('/')
        if fs in tree:
            if cfg not in tree[fs]:
                tree[fs][cfg] = {}
        else:
            tree[fs] = {cfg: {}}

        # Parse the test results
        for test_case in root.iter('testcase'):
            if test_case.find('skipped') is not None:
                result = 'skip'
            elif test_case.find('failure') is not None:
                result = 'fail'
            else:
                result = 'pass'
            suite, test = test_case.attrib['name'].split('/')
            if not suite in tree[fs][cfg]:
                tree[fs][cfg][suite] = {}
            tree[fs][cfg][suite][test] = {
                'result'   : result,
                # Save the path of the original XML results file as test
                # artifacts
                'artifacts': {'log': os.path.join(
                    self.artifacts_dir, os.path.basename(f))},
            }
        return tree

    def _convert_results_for_api(self, tree, name):
        """Takes a tree containing the fstests results and formats it as
        KernelCI-API nodes.

        Args:
            tree (dict): fstests results
            name (str): name of the result root node

        Returns:
            A dict containing the KernelCI API nodes ready to be
            submitted.
        """
        if 'result' in tree:
            return {
                'node': {
                    'name': name,
                    'result': tree['result'],
                    'artifacts': tree.get('artifacts'),
                },
                'child_nodes':[]
            }
        else:
            child_nodes = []
            for child_name in tree:
                child_nodes.append(
                    self._convert_results_for_api(tree[child_name], child_name))
            return {
                'node':{
                    'name': name,
                },
                'child_nodes': child_nodes,
            }

    def upload_artifacts(self, path):
        """Uploads the artifact and log files stored in <path> to KernelCI
        storage.

        Args:
            path (str): local directory that contains the test artifacts
            and log files.

        NOTES:
            - this currently uses scp to upload the files.
            - it uses the SSH_HOST, SSH_KEY, SSH_PORT and SSH_USER
              options for the scp command
            - it will raise an exception if the scp command fails
        """
        path = os.path.normpath(path)
        dest_basepath = f'{SSH_USER}@{SSH_HOST}:~/data'
        r = subprocess.run(['scp',
                            '-i', SSH_KEY,
                            '-P', SSH_PORT,
                            '-o', 'StrictHostKeyChecking=no',
                            '-o', 'UserKnownHostsFile=/dev/null',
                            '-r',
                            path,
                            dest_basepath],
                           check=True,
                           capture_output=True)
        self.artifacts_dir = os.path.join(dest_basepath,
                                          os.path.basename(path))

    def parse_results(self, path):
        """Parses and processes the test result files in PATH.

        Args:
            path (str): Directory to look for XML-formatted result files

        Returns:
            A dict containing the test results formatted according to the
            KernelCI-API.
        """
        # Parse the test results
        test_results = {}
        for xmlfile in glob.glob(os.path.join(path, 'results-*.xml')):
            test_results = self._parse_xml_test_results(xmlfile, test_results)

        # Turn them into KernelCI-API data
        api_data = self._convert_results_for_api(test_results, 'fstests')
        return api_data

    @abstractmethod
    def run(self, kdir, testcase='', group=False, testcfg=''):
        """Runs the specified fstests testcase using the kernel built in KDIR.

        Args:
            kdir (str): directory containing a built Linux kernel
            testcase (str): fstests to run (single test or
                comma-separated list)
            group (bool): true if the testcase argument refers to a test group
            testcfg (str): fstests config to run (single config or
                comma-separated list)

        Returns:
            bool: True for success, False otherwise
        """
        pass

    @abstractmethod
    def get_xml_results(self, path):
        """Extracts the XML-formatted test results and stores them in PATH.

        Args:
            path (str): Directory where to copy the XML results file.

        Returns:
                bool: True for success, False otherwise
        """
        pass


class KvmFstests(Fstests):
    # fstests fs configs (key = config name, value = results path)
    configs = {'4k'            : 'ext4/results-4k',
               '1k'            : 'ext4/results-1k',
               'ext3'          : 'ext4/results-ext3',
               'encrypt'       : 'ext4/results-encrypt',
               'nojournal'     : 'ext4/results-nojournal',
               'ext3conv'      : 'ext4/results-ext3conv',
               'adv'           : 'ext4/results-adv',
               'dioread_nolock': 'ext4/results-dioread_nolock',
               'data_journal'  : 'ext4/results-data_journal',
               'bigalloc'      : 'ext4/results-bigalloc',
               'bigalloc_1k'   : 'ext4/results-bigalloc_1k',
    }

    def __del__(self):
        """Clean up temporary resources."""
        # Remove all the results.xml files from the results virtual disk (vdg)
        for path in self.configs.values():
            xml_path = os.path.join(path, 'results.xml')
            subprocess.run((f'debugfs -wR "rm {xml_path}" '
                            f'{XFSTESTS_BLD_PATH}/run-fstests/disks/vdg'),
                           shell=True,
                           check=True,
                           capture_output=True)

    def run(self, kernel_path, testcase='', group=False, testcfg=''):
        cmd = ['kvm-xfstests']
        if testcfg:
            cmd.extend(['-c', testcfg.replace(' ', '')])
        if group:
            cmd.extend(['-g', testcase.replace(' ', '')])
        else:
            cmd.append(testcase.replace(' ', ''))
        if os.path.isfile(kernel_path):
            cmd.extend(['--kernel', kernel_path])
        else:
            try:
                os.chdir(kernel_path)
            except FileNotFoundError:
                print(f"Kernel directory not found: {kernel_path}")
                return False
        try:
            result = subprocess.run(cmd, check=True)
            print(f"Test run success.")
            return True
        except Exception as e:
            print("Problem found during tests:", e)
            return False

    def get_xml_results(self, path):
        try:
            for k, v in self.configs.items():
                xml_path = os.path.join(v, 'results.xml')
                result = subprocess.run([
                    'debugfs', '-R', f'dump {xml_path} {path}/results-{k}.xml',
                    f'{XFSTESTS_BLD_PATH}/run-fstests/disks/vdg'],
                    check=True,
                    capture_output=True)
            return True
        except Exception as e:
            # TBD: proper error handling
            traceback.print_exc()
            return False


class GceFstests(Fstests):
    # fstests fs configs (key = config name, value = results path)
    configs = {'4k'            : 'ext4/results-4k',
               '1k'            : 'ext4/results-1k',
               'ext3'          : 'ext3/results-default',
               'encrypt'       : 'ext4/results-encrypt',
               'nojournal'     : 'ext4/results-nojournal',
               'ext3conv'      : 'ext4/results-ext3conv',
               'adv'           : 'ext4/results-adv',
               'dioread_nolock': 'ext4/results-dioread_nolock',
               'data_journal'  : 'ext4/results-data_journal',
               'bigalloc'      : 'ext4/results-bigalloc',
               'bigalloc_1k'   : 'ext4/results-bigalloc_1k',
    }

    def __init__(self, max_shards=None):
        self.max_shards = max_shards
        # List of dicts. Each list element represents a shard (a
        # distinct VM instance). The dict keys are:
        #  - 'id': a unique numeric suffix for each shard in a group, starting from 0
        #  - 'configs': list of fs configs to run in the shard
        #  - 'results_tarball': GS bucket path of the results tarball
        #  - 'unpacked_results': local path where the results are unpacked
        #  - 'thread': thread that polls the VM
        self.shards = []
        self.kernelimg = ''
        # NOTE: this script will most likely be launched as a subprocess
        # (see kernelci-core/kernelci/lab/shell.py), so these variables
        # should be __exported__ in order to be visible here.
        self.gce_env = os.environ.copy()
        if 'GCE_MACHTYPE' not in self.gce_env:
            self.gce_env['GCE_MACHTYPE'] = GCE_DEFAULT_MACHTYPE
        if GCE_PROJECT:
            self.gce_env['GCE_PROJECT'] = GCE_PROJECT
        if GCE_ZONE:
            self.gce_env['GCE_ZONE'] = GCE_ZONE
        if GS_BUCKET:
            self.gce_env['GS_BUCKET'] = GS_BUCKET
        # Generate a (hopefully) unique name for the VM ID and results file
        self.uniqstr = '{}-{}'.format(
            datetime.datetime.now().strftime('%Y%m%d%H%M%S'),
            ''.join(random.choice(string.ascii_lowercase) for i in range(8)))

    def __del__(self):
        """Clean up local and remote temporary resources."""
        # Delete kernel image stored in the GS bucket
        if self.kernelimg:
            subprocess.run(['gsutil', 'rm', self.kernelimg],
                           check=True,
                           env=self.gce_env,
                           capture_output=True,
                           encoding='utf-8')
        for shard in self.shards:
            # Delete shard logs in the GS bucket
            if 'results_tarball' in shard:
                subprocess.run(['gsutil', 'rm', shard['results_tarball']],
                               check=True,
                               env=self.gce_env,
                               capture_output=True,
                               encoding='utf-8')
            # Delete local temporary files: unpacked results and results tarball.
            # The tarball should be named something like
            #     /tmp/results.<timestamp>-<unique-string>.<kernel_version>.tar.xz
            # The directory containing the unpacked results will be:
            #     /tmp/results-<timestamp>-<unique-string>
            if 'unpacked_results' in shard:
                shutil.rmtree(shard['unpacked_results'])
                packed_results = shard['unpacked_results'].replace('-', '.', 1)
                for f in glob.glob(f'{packed_results}.*'):
                    os.remove(f)

    def _get_quotas(self):
        """Gets the quotas for the GCE region that contains
        self.gce_enc['GCE_ZONE'].

        Returns:
            A dict with keys = resource name, and values = quota,
            None if failure
        """
        quotas = {}
        # Get the region from the GCE_ZONE: the zone could be named as a
        # region with an additional suffix
        region = re.match('(\w+-\w+)', self.gce_env['GCE_ZONE']).group()
        if not region:
            print(f"Error: can't get GCE region from zone: {self.gce_env['GCE_ZONE']}")
            return None
        try:
            result = subprocess.run(
                ['gcloud', 'compute', 'regions', 'describe', region],
                check=True,
                capture_output=True,
                encoding='utf-8')
        except Exception as e:
            print(f"Error getting project quotas for region {region}: {str(e)}")
            return None
        # Discard output header
        lines = result.stdout.splitlines()[6:]
        # The quotas output of 'gcloud compute regions describe <region>'
        # looks like this for each metric:
        # - limit: <float>
        #   metric: <string>
        #   usage: <float>
        limit = 0
        metric = ''
        for l in lines:
            m = re.match('\W+(\w+): (.*)', l)
            if m:
                if m.group(1) == 'limit':
                   limit = float(m.group(2))
                elif m.group(1) == 'metric':
                   metric = m.group(2)
                elif m.group(1) == 'usage':
                   quotas[metric] = limit - float(m.group(2))
        return quotas

    def _get_machtype_cpus(self, machtype):
        """Retrieves the number of CPUs for a specific GCE machine type.

        Args:
            machtype (str): the machine type to query

        Returns:
            The number of CPUs of the specified machine type (int)
            None if failure
        """
        try:
            result = subprocess.run(['gcloud', 'compute', 'machine-types',
                                     'describe', machtype],
                                    check=True,
                                    env=self.gce_env,
                                    capture_output=True,
                                    encoding='utf-8')
        except Exception as e:
            print(f"Error getting the number of CPUs for machine type {machtype}: {str(e)}")
            return None
        m = re.search("guestCpus: (\d+)", result.stdout)
        if not m:
            print((f"Error getting the number of CPUs for machine {machtype}: "
                   f"{result.stdout}"))
            return None
        return int(m.group(1))

    def _wait_for_completion(self):
        """Checks the existence of a test result tarball for VM_ID in the Google
        Storage bucket specified in the GS_BUCKET parameter. This method
        blocks until the result is generated or until a
        KeyboardInterrupt is received.

        Returns:
            (str) The path of the results tarball as a GS link.
        """
        def poll_shard(shard):
            pattern = f'gs://{self.gce_env["GS_BUCKET"]}/results/results.{self.uniqstr}-{shard["id"]}*'
            print(f"wait for result: {pattern}")
            while True:
                result = subprocess.run(
                    ['gsutil', 'ls', pattern],
                    capture_output=True,
                    encoding='utf-8')
                if result.returncode == 0:
                    shard['results_tarball'] = result.stdout
                    return
                time.sleep(POLL_PERIOD)

        # Poll all threads in parallel
        for shard in self.shards:
            if not 'configs' in shard:
                break
            shard['thread'] = threading.Thread(target=poll_shard,
                                               args=(shard,), daemon=True)
            shard['thread'].start()
        # Wait for all threads to finish
        try:
            for shard in self.shards:
                if not 'thread' in shard:
                    break
                shard['thread'].join()
        except KeyboardInterrupt:
            return False
        return True

    def _upload_kernel(self, kernel_path):
        """Uploads a kernel binary image to the GS bucket
        (self.gce_env['GS_BUCKET']) with a unique file name.

        Args:
            kernel_path (str): path to the kernel, either as an image or
            as a decompressed source tree with a built kernel in it.

        Returns:
            The location of the uploaded file as a GS link (str) or None
            in case of failure.
        """
        cmd = ['gce-xfstests', 'upload-kernel']
        if os.path.isfile(kernel_path):
            cmd.extend(['--kernel', kernel_path])
        else:
            try:
                os.chdir(kernel_path)
            except FileNotFoundError:
                print(f"Kernel directory not found: {kernel_path}")
                return None
        filename = f'kernels/bzImage-{self.uniqstr}'
        cmd.append(filename)
        try:
            subprocess.run(cmd, check=True,
                           env=self.gce_env,
                           capture_output=True,
                           encoding='utf-8')
        except Exception as e:
            print((f"Error uploading the kernel image: {str(e)}"))
            return None
        return f'gs://{self.gce_env["GS_BUCKET"]}/{filename}'

    def run(self, kernel_path, testcase='', group=False, testcfg=''):
        # Calculate the number of parallel shards depending on the
        # region quotas
        quotas = self._get_quotas()
        if not quotas:
            print("Error retrieving GCE quotas")
            return False
        num_shards = int(min(quotas['CPUS'],
                             quotas['IN_USE_ADDRESSES'],
                             quotas['SSD_TOTAL_GB']))

        # Shard requirements: Every shard needs 2 vCPUs and SSD space of
        # GCE_MIN_SCR_SIZE (100GB), see
        # https://github.com/tytso/xfstests-bld/blob/34e81de235cf380d30082779aef5e6f50b9b9548/test-appliance/files/usr/local/lib/gce-server/util/gcp/gcp.go#L124
        # and
        # https://github.com/tytso/xfstests-bld/blob/34e81de235cf380d30082779aef5e6f50b9b9548/run-fstests/config.gce#L36
        cpus = self._get_machtype_cpus(self.gce_env['GCE_MACHTYPE'])
        if not cpus:
            return False
        num_shards = int(min(quotas['IN_USE_ADDRESSES'],
                             quotas['CPUS'] / cpus,
                             quotas['SSD_TOTAL_GB'] / 100))
        for n in range(0, num_shards):
            self.shards.append({'id': n})
        if num_shards < 1:
            print("GCE quota exceeded, can't start a VM")
            return False
        if self.max_shards != None and self.max_shards < num_shards:
            num_shards = self.max_shards

        # Distribute the test configs into shards. If no test configs
        # are specified, consider all available configs
        if not testcfg:
            cfgs = self.configs.keys()
        else:
            cfgs = re.split(r',', testcfg.replace(' ', ''))
        cfgs_per_shard = math.ceil(len(cfgs) / num_shards)
        shard_idx = 0
        for c in cfgs:
            if 'configs' in self.shards[shard_idx]:
                self.shards[shard_idx]['configs'].append(c)
            else:
                self.shards[shard_idx]['configs'] = [c]
            shard_idx = (shard_idx + 1) % num_shards

        self.kernelimg = self._upload_kernel(kernel_path)
        if not self.kernelimg:
            print((f"Error uploading the kernel image ({kernel_path}) "
                   f"to the GS bucket {self.gce_env['GS_BUCKET']}"))
            return False

        for shard in self.shards:
            if not 'configs' in shard:
                break
            cmd = ['gce-xfstests']
            cmd.extend(['--kernel', self.kernelimg])
            vm_id = f'xfstests-{self.uniqstr}-{shard["id"]}'
            cmd.extend(['--instance-name', vm_id])
            cmd.extend(['-c', ','.join(shard['configs'])])
            if group:
                cmd.extend(['-g', testcase.replace(' ', '')])
            else:
                cmd.append(testcase.replace(' ', ''))
            try:
                result = subprocess.run(cmd, check=True,
                                        env=self.gce_env,
                                        capture_output=True, encoding='utf-8')
                print(f"Test started in VM {vm_id}. Waiting for it to finish")
            except FileNotFoundError:
                print("Can't find the gce-xfstests script, check $PATH")
                return False
            except subprocess.CalledProcessError as e:
                print("Error running gce-xfstests:", e.stdout)
                return False
            except Exception:
                traceback.print_exc()
                return False
        return self._wait_for_completion()


    def get_xml_results(self, path):
        # Get the results from the GS bucket (defined in the GS_BUCKET
        # env var) using `gce-xfstests get-results --unpack`.
        # That will download the results tarball to a temp dir and
        # decompress it.
        for shard in self.shards:
            if 'results_tarball' not in shard:
                break
            try:
                result = subprocess.run(
                    ['gce-xfstests', 'get-results', '--unpack',
                     os.path.basename(shard['results_tarball'])],
                    check=True, env=self.gce_env,
                    capture_output=True, encoding='utf-8')
                m = re.match("Unpacked results at (.+)$", result.stdout)
                if not m:
                    print(f"Error unpacking results: {result.stdout}")
                    return False
                shard['unpacked_results'] = m.group(1)
            except subprocess.CalledProcessError as e:
                print("Error running gce-xfstests get-results:", e.stdout)
                return False
            except Exception:
                traceback.print_exc()
                return False

            # Copy the XML result files for all test configs to <path>
            for c in shard['configs']:
                result_file = os.path.join(shard['unpacked_results'],
                                           self.configs[c],
                                           'results.xml')
                try:
                    shutil.copy(result_file, os.path.join(path, f'results-{c}.xml'))
                except FileNotFoundError:
                    print(f"File not found: {result_file}")
                    return False
            print(f"XML files extracted to {path}")
        return True


class Job(BaseJob):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Use a unique workspace dir for each run
        # NOTE: self._workspace_dir is only used to track the object
        # lifetime properly
        self._workspace_dir = tempfile.TemporaryDirectory(dir=self._workspace)
        self._workspace = self._workspace_dir.name

    def _check_parameters(self):
        if not SSH_HOST:
            print("ssh_host parameter not specified")
            return False
        if not SSH_KEY :
            print("ssh_key parameter not specified")
            return False
        if not SSH_PORT:
            print("ssh_port parameter not specified")
            return False
        if not SSH_USER:
            print("ssh_user parameter not specified")
            return False
        if not OUTPUT:
            print("output parameter not specified")
            return False
        if TESTCASE and TESTGROUP:
            print("Error, can't specify both the testcase and the testgroup.")
            return False
        if not TESTCASE and not TESTGROUP:
            print("Error, no testcase or testgroup specified.")
            return False
        return True

    def _run(self, kernel_path):
        fail_results = {
            'node': {
                'result': 'fail',
                'state': 'done',
            },
            'child_nodes':[],
        }
        if not self._check_parameters():
             return fail_results

        if GCE and GCE == 'True':
            max_shards = None
            if MAX_SHARDS:
                max_shards = int(MAX_SHARDS)
            test = GceFstests(max_shards)
        else:
            test = KvmFstests()

        # Config and build kernel
        # TBD: Skip if using a prebuilt kernel
        if not all([test.kernel_config(kernel_path),
                    test.kernel_build(kernel_path, NJOBS)]):
            return fail_results
        if TESTGROUP:
            test_ok = test.run(kernel_path, TESTGROUP, True, TESTCFG)
        else:
            test_ok = test.run(kernel_path, TESTCASE, False, TESTCFG)
        if not test_ok:
            return fail_results

        if not test.get_xml_results(OUTPUT):
            return fail_results
        try:
            test.upload_artifacts(OUTPUT)
            results = test.parse_results(OUTPUT)
            # TBD: What does 'pass' mean in the root node?
            results['node']['result'] = 'pass'
            results['node']['state'] = 'done'
            with open(os.path.join(OUTPUT, 'results.json'), 'w') as result_file:
                result_file.write(json.dumps(results))
        except Exception as e:
            traceback.print_exc()
            return fail_results
        return results

    def _submit(self, results, node_id, db):
        node = db.get_node(node_id)
        results['node']['name'] = node['name']
        db.submit_results(results, node)
        print(f"Result node submitted: {node_id}")
        return node
{% endblock %}
